
# LogGD Model Flow

## 1. Input
- **Log Data**: The input to the model is **log sequences** generated by the system. Each log contains events and timestamps that record system states or actions.
- The **log sequences** are transformed into a **graph**:
  - **Each event** in the log is a **node**.
  - **Edges between nodes** represent the relationship between log events (i.e., which event follows another in the sequence).
  - **Edge weights** indicate the frequency of co-occurring events.

## 2. Encoding
The encoding process includes two main components:
- **Node Embedding**:
  - Each node in the graph (corresponding to a log event) is **semantically encoded** using NLP techniques like **BERT**, creating an embedding vector.
  - Semantic features help the model understand the context and meaning of log events.
- **Structure-aware Encoding**:
  - Besides semantic features, LogGD also uses **structural attributes** of the graph. These include:
    1. **Degree matrix**: Reflecting the connectivity of nodes (in-degree, out-degree).
    2. **Distance matrix**: Encoding node distances based on the shortest paths.
    3. **Edge weight matrix**: Encoding edge weights, which represent the frequency of event connections.
  - The combination of semantic and structural features creates a multi-dimensional encoding that captures the complex relationships in log data.

## 3. Feature Learning
- The model uses **Graph Transformer Neural Network (GTNN)** to **learn features** from the graph:
  - **Self-attention**: The model uses a self-attention mechanism to determine the importance of each node in the graph based on its relationship with neighboring nodes, updating the node representations by aggregating information from neighbors.
  - **Node-structure interaction**: The model combines node feature information with graph structure information to better understand the relationships between log events and detect anomalies.

## 4. Output
- After processing through the GTNN layers, LogGD creates a **graph-level representation** via the **READOUT** function, using methods like **sum** or **max pooling** to aggregate all nodes into a representative vector.
- **Classification**: This graph-level representation is fed into a **feed-forward network** to classify whether the log sequence is **anomalous** or **normal**.
  - The model uses a **softmax function** to compute probabilities and make the final prediction.

---

### Summary of Model Flow:
1. **Log sequences** are transformed into a **graph**.
2. **Node features** are encoded using semantics and graph structure.
3. **Features are learned** using Graph Transformer.
4. **Graph-level representation** is built and used for anomaly detection.

By combining **semantic** and **graph structural** information, LogGD can detect anomalies more accurately compared to conventional quantitative or sequence-based methods.
